# Multimodal Classification: Cats vs Dogs

基于视觉和音频的多模态分类系统，模拟人类如何同时处理视觉和听觉信息来识别猫狗。

## 项目概述

本项目实现了一个多模态深度学习模型，结合视觉（图像）和听觉（音频）信息对猫狗进行分类。模型采用预训练的 ResNet-50 处理图像，Wav2Vec2 处理音频，通过冻结主干网络并训练投影层和分类器来实现高效的多模态融合。

### 主要特性

- **多模态融合**: 同时处理图像和音频输入
- **灵活推理**: 支持单模态（仅图像/仅音频）和混合模态推理
- **训练时数据增强**: 随机遮挡视觉/听觉通路，提高模型鲁棒性
- **可视化分析**: 权重热图、性能曲线、神经元选择性分析
- **TensorBoard 集成**: 实时监控训练过程

## 模型架构

```
输入:
├── 图像 (224x224) → ResNet-50 (冻结) → 投影层 (2048→512)
└── 音频 (16kHz)   → Wav2Vec2 (冻结) → 投影层 (768→512)
                                              ↓
                                    融合特征 (1024)
                                              ↓
                                    分类器 (MLP: 1024→256→2)
                                              ↓
                                         预测类别
```

### 关键组件

- **视觉编码器**: `microsoft/resnet-50` - 预训练的 ResNet-50 模型
- **音频编码器**: `facebook/wav2vec2-base` - 预训练的 Wav2Vec2 模型
- **投影层**: 将不同维度的特征映射到共享空间 (512维)
- **分类器**: 2层 MLP，最终输出猫/狗二分类结果


## 项目结构

```
neuron/
├── model.py              # 多模态模型定义
├── dataset.py            # 数据加载和预处理
├── mix_dataset.py        # 配对数据集生成器
├── train.py              # 训练脚本
├── fine_tune.py          # 单模态微调脚本
├── inference.py          # 推理和可视化函数
├── requirements.txt      # 项目依赖
├── checkpoints/          # 模型检查点
├── logs/                 # TensorBoard 日志
├── resnet-cats-dogs2/    # 微调后的图像模型
└── wav2vec2-cats-dogs/   # 微调后的音频模型
```


### 关键发现

1. **模态互补性**: 混合模态的性能优于任一单模态，说明视觉和听觉信息具有互补性
2. **神经元选择性**: 部分神经元对特定模态表现出偏好（通过权重分析发现）
3. **鲁棒性**: 训练时的随机遮挡策略提高了模型对单模态缺失的鲁棒性



## 未来工作
- [x] 查看模型行为是否符合贝叶斯
- [x] 实现交叉注意力融合机制
- [x] 因果推断相关实验


## 许可证

MIT License

## 联系方式

如有问题或建议，欢迎提交 Issue 或 Pull Request。
